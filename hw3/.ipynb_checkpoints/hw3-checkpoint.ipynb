{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network on MNIST Dataset\n",
    "## Krishna Thiyagarajan\n",
    "## ECE - 411 - Computational Graphs for Machine Learning\n",
    "## Professor Chris Curro\n",
    "## Homework Assignment #3a\n",
    "## February 18, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "num_inputs = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def def_weight(shape, name):\n",
    "    var = tf.get_variable(name = name, dtype = tf.float32, shape = shape, initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('model_vars', var)\n",
    "    tf.add_to_collection('l2', tf.reduce_sum(tf.square(var)))\n",
    "    return var\n",
    "\n",
    "def def_bias(shape, name):\n",
    "    var = tf.get_variable(name = name, dtype = tf.float32, shape = shape, initializer = tf.constant_initializer(0.0))\n",
    "    tf.add_to_collection('model_vars', var)\n",
    "    tf.add_to_collection('l2', tf.reduce_sum(tf.square(var)))\n",
    "    return var\n",
    "\n",
    "class ConvANN:\n",
    "    def __init__(self, sess, weight_dim, bias_dim, iterations, batch_size, learn_rate, gamma,  display_steps=100, num_in=num_inputs, num_class=num_classes):\n",
    "        self.sess = sess\n",
    "        self.num_inputs = num_in\n",
    "        self.num_classes = num_class\n",
    "        self.weight_dims = weight_dim\n",
    "        self.bias_dims = bias_dim\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.display_steps = display_steps\n",
    "        self.learn_rate = learn_rate\n",
    "        self.gamma = gamma\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.num_inputs])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "        self.build_model()\n",
    "\n",
    "    def conv2d(self, x, w, b, stride=1):\n",
    "        x = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def maxpool2d(self, x, k=2):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "    def build_model(self):\n",
    "        x = tf.reshape(self.x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        self.yhat = self.conv2d(x, self.weight_dims['w1'], self.bias_dims['b1'])\n",
    "        self.yhat = self.maxpool2d(self.yhat)\n",
    "        self.yhat = self.conv2d(self.yhat, self.weight_dims['w2'], self.bias_dims['b2'])\n",
    "        self.yhat = self.maxpool2d(self.yhat)\n",
    "\n",
    "        self.yhat = tf.reshape(self.yhat, [-1, self.weight_dims['w3'].get_shape().as_list()[0]])\n",
    "        self.yhat = tf.add(tf.matmul(self.yhat, self.weight_dims['w3']), self.bias_dims['b3'])\n",
    "        self.yhat = tf.nn.relu(self.yhat)\n",
    "\n",
    "        self.yhat = tf.nn.dropout(self.yhat, self.dropout)\n",
    "\n",
    "        self.yhat = tf.add(tf.matmul(self.yhat, self.weight_dims['w4']), self.bias_dims['b4'])\n",
    "\n",
    "        self.costs = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.yhat, labels=self.y))\n",
    "        self.l2 = tf.reduce_sum(tf.get_collection('l2'))\n",
    "        self.loss = self.costs + self.gamma * self.l2\n",
    "\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.yhat, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "\n",
    "    def train(self):\n",
    "    \tmodel_vars = tf.get_collection('model_vars')\n",
    "    \tself.optim = (tf.train.AdamOptimizer(learning_rate=self.learn_rate).minimize(self.loss, var_list=model_vars))\n",
    "    \tself.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \tfor kk in range(self.iterations):\n",
    "    \t\tbatch_x, batch_y = mnist.train.next_batch(self.batch_size)\n",
    "    \t\tself.sess.run([self.optim], feed_dict={self.x: batch_x, self.y: batch_y, self.dropout: 0.75})\n",
    "    \t\tif kk % self.display_steps == 0:\n",
    "    \t\t\tloss = self.sess.run(self.loss, feed_dict={self.x: batch_x, self.y: batch_y, self.dropout: 1.0})\n",
    "    \t\t\tprint(\"Step: %d, Loss: %f\" % (kk, loss))\n",
    "    \tprint(\"Optimization complete!\")\n",
    "    \tself.valid_accuracy()\n",
    "\n",
    "    def valid_accuracy(self):\n",
    "    \tacc = self.sess.run(self.accuracy, feed_dict={self.x: mnist.validation.images[:1000], self.y: mnist.validation.labels[:1000], self.dropout: 1.0})\n",
    "    \tprint(\"Validation Accuracy: \", acc)\n",
    "\n",
    "    def test_accuracy(self):\n",
    "    \tacc = self.sess.run(self.accuracy, feed_dict={self.x: mnist.test.images[:500], self.y: mnist.test.labels[:500], self.dropout: 1.0})\n",
    "    \tprint(\"Test Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.215885\n",
      "Step: 100, Loss: 0.190638\n",
      "Step: 200, Loss: 0.196720\n",
      "Step: 300, Loss: 0.160326\n",
      "Step: 400, Loss: 0.219079\n",
      "Step: 500, Loss: 0.278651\n",
      "Step: 600, Loss: 0.204617\n",
      "Step: 700, Loss: 0.118456\n",
      "Step: 800, Loss: 0.095850\n",
      "Step: 900, Loss: 0.120182\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.985\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "\n",
    "sess_1 = tf.Session()\n",
    "\n",
    "weight_dim_1 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w11'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w12'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w13'),\n",
    "    'w4': def_weight([1024, num_classes], 'w14')\n",
    "}\n",
    "\n",
    "bias_dim_1= {\n",
    "    'b1': def_bias([32], 'b11'),\n",
    "    'b2': def_bias([64], 'b12'),\n",
    "    'b3': def_bias([1024], 'b13'),\n",
    "    'b4': def_bias([num_classes], 'b14')\n",
    "}\n",
    "\n",
    "runs_1=1000\n",
    "\n",
    "minibatch_1 = 64\n",
    "learnRate_1 = 1e-3\n",
    "gamma_1 = 1e-4\n",
    "\n",
    "\n",
    "model_1 = ConvANN(sess_1, weight_dim_1, bias_dim_1, runs_1, minibatch_1, learnRate_1, gamma_1)\n",
    "\n",
    "model_1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.393782\n",
      "Step: 100, Loss: 0.384075\n",
      "Step: 200, Loss: 0.132534\n",
      "Step: 300, Loss: 0.140111\n",
      "Step: 400, Loss: 0.181190\n",
      "Step: 500, Loss: 0.134737\n",
      "Step: 600, Loss: 0.128377\n",
      "Step: 700, Loss: 0.280562\n",
      "Step: 800, Loss: 0.069391\n",
      "Step: 900, Loss: 0.108024\n",
      "Step: 1000, Loss: 0.105062\n",
      "Step: 1100, Loss: 0.083044\n",
      "Step: 1200, Loss: 0.093895\n",
      "Step: 1300, Loss: 0.084782\n",
      "Step: 1400, Loss: 0.068128\n",
      "Step: 1500, Loss: 0.067210\n",
      "Step: 1600, Loss: 0.069318\n",
      "Step: 1700, Loss: 0.061077\n",
      "Step: 1800, Loss: 0.188755\n",
      "Step: 1900, Loss: 0.076858\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.981\n"
     ]
    }
   ],
   "source": [
    "# Run 2\n",
    "\n",
    "sess_2 = tf.Session()\n",
    "\n",
    "weight_dim_2 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w21'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w22'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w23'),\n",
    "    'w4': def_weight([1024, num_classes], 'w24')\n",
    "}\n",
    "\n",
    "bias_dim_2= {\n",
    "    'b1': def_bias([32], 'b21'),\n",
    "    'b2': def_bias([64], 'b22'),\n",
    "    'b3': def_bias([1024], 'b23'),\n",
    "    'b4': def_bias([num_classes], 'b24')\n",
    "}\n",
    "\n",
    "runs_2=2000\n",
    "\n",
    "minibatch_2 = 64\n",
    "learnRate_2 = 1e-3\n",
    "gamma_2 = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "model_2 = ConvANN(sess_2, weight_dim_2, bias_dim_2, runs_2, minibatch_2, learnRate_2, gamma_2)\n",
    "\n",
    "model_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.447249\n",
      "Step: 100, Loss: 0.333082\n",
      "Step: 200, Loss: 0.302939\n",
      "Step: 300, Loss: 0.151420\n",
      "Step: 400, Loss: 0.200318\n",
      "Step: 500, Loss: 0.223454\n",
      "Step: 600, Loss: 0.103670\n",
      "Step: 700, Loss: 0.103654\n",
      "Step: 800, Loss: 0.117292\n",
      "Step: 900, Loss: 0.081809\n",
      "Step: 1000, Loss: 0.102687\n",
      "Step: 1100, Loss: 0.076788\n",
      "Step: 1200, Loss: 0.091380\n",
      "Step: 1300, Loss: 0.076963\n",
      "Step: 1400, Loss: 0.148603\n",
      "Step: 1500, Loss: 0.086423\n",
      "Step: 1600, Loss: 0.097505\n",
      "Step: 1700, Loss: 0.091366\n",
      "Step: 1800, Loss: 0.093944\n",
      "Step: 1900, Loss: 0.074915\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.985\n"
     ]
    }
   ],
   "source": [
    "# Run 3\n",
    "\n",
    "sess_3 = tf.Session()\n",
    "\n",
    "weight_dim_3 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w31'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w32'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w33'),\n",
    "    'w4': def_weight([1024, num_classes], 'w34')\n",
    "}\n",
    "\n",
    "bias_dim_3= {\n",
    "    'b1': def_bias([32], 'b31'),\n",
    "    'b2': def_bias([64], 'b32'),\n",
    "    'b3': def_bias([1024], 'b33'),\n",
    "    'b4': def_bias([num_classes], 'b34')\n",
    "}\n",
    "\n",
    "runs_3=2000\n",
    "\n",
    "minibatch_3 = 32\n",
    "learnRate_3 = 1e-3\n",
    "gamma_3 = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "model_3 = ConvANN(sess_3, weight_dim_3, bias_dim_3, runs_3, minibatch_3, learnRate_3, gamma_3)\n",
    "\n",
    "model_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 13.727271\n",
      "Step: 100, Loss: 2.476243\n",
      "Step: 200, Loss: 2.439242\n",
      "Step: 300, Loss: 2.346537\n",
      "Step: 400, Loss: 2.344593\n",
      "Step: 500, Loss: 2.330914\n",
      "Step: 600, Loss: 2.309442\n",
      "Step: 700, Loss: 2.333476\n",
      "Step: 800, Loss: 2.326510\n",
      "Step: 900, Loss: 2.280150\n",
      "Step: 1000, Loss: 2.313780\n",
      "Step: 1100, Loss: 2.300328\n",
      "Step: 1200, Loss: 2.294148\n",
      "Step: 1300, Loss: 2.298687\n",
      "Step: 1400, Loss: 2.293363\n",
      "Step: 1500, Loss: 2.323333\n",
      "Step: 1600, Loss: 2.313638\n",
      "Step: 1700, Loss: 2.307415\n",
      "Step: 1800, Loss: 2.304341\n",
      "Step: 1900, Loss: 2.333972\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.116\n"
     ]
    }
   ],
   "source": [
    "# Run 4\n",
    "\n",
    "sess_4 = tf.Session()\n",
    "\n",
    "weight_dim_4 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w41'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w42'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w43'),\n",
    "    'w4': def_weight([1024, num_classes], 'w44')\n",
    "}\n",
    "\n",
    "bias_dim_4= {\n",
    "    'b1': def_bias([32], 'b41'),\n",
    "    'b2': def_bias([64], 'b42'),\n",
    "    'b3': def_bias([1024], 'b43'),\n",
    "    'b4': def_bias([num_classes], 'b44')\n",
    "}\n",
    "\n",
    "runs_4 = 2000\n",
    "\n",
    "minibatch_4 = 32\n",
    "learnRate_4 = 1e-2\n",
    "gamma_4 = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "model_4 = ConvANN(sess_4, weight_dim_4, bias_dim_4, runs_4, minibatch_4, learnRate_4, gamma_4)\n",
    "\n",
    "model_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 5.038886\n",
      "Step: 100, Loss: 2.417109\n",
      "Step: 200, Loss: 2.399310\n",
      "Step: 300, Loss: 2.324419\n",
      "Step: 400, Loss: 2.320112\n",
      "Step: 500, Loss: 2.325422\n",
      "Step: 600, Loss: 2.309214\n",
      "Step: 700, Loss: 2.355353\n",
      "Step: 800, Loss: 2.311794\n",
      "Step: 900, Loss: 2.284373\n",
      "Step: 1000, Loss: 2.277915\n",
      "Step: 1100, Loss: 2.282344\n",
      "Step: 1200, Loss: 2.266680\n",
      "Step: 1300, Loss: 2.326157\n",
      "Step: 1400, Loss: 2.275277\n",
      "Step: 1500, Loss: 2.317041\n",
      "Step: 1600, Loss: 2.298300\n",
      "Step: 1700, Loss: 2.336502\n",
      "Step: 1800, Loss: 2.312979\n",
      "Step: 1900, Loss: 2.305370\n",
      "Step: 2000, Loss: 2.305101\n",
      "Step: 2100, Loss: 2.320704\n",
      "Step: 2200, Loss: 2.346688\n",
      "Step: 2300, Loss: 2.250865\n",
      "Step: 2400, Loss: 2.339192\n",
      "Step: 2500, Loss: 2.337123\n",
      "Step: 2600, Loss: 2.277326\n",
      "Step: 2700, Loss: 2.256661\n",
      "Step: 2800, Loss: 2.325802\n",
      "Step: 2900, Loss: 2.287292\n",
      "Step: 3000, Loss: 2.298874\n",
      "Step: 3100, Loss: 2.287511\n",
      "Step: 3200, Loss: 2.290134\n",
      "Step: 3300, Loss: 2.319763\n",
      "Step: 3400, Loss: 2.320627\n",
      "Step: 3500, Loss: 2.295411\n",
      "Step: 3600, Loss: 2.293262\n",
      "Step: 3700, Loss: 2.314299\n",
      "Step: 3800, Loss: 2.297563\n",
      "Step: 3900, Loss: 2.303945\n",
      "Step: 4000, Loss: 2.343799\n",
      "Step: 4100, Loss: 2.285847\n",
      "Step: 4200, Loss: 2.298385\n",
      "Step: 4300, Loss: 2.306934\n",
      "Step: 4400, Loss: 2.282217\n",
      "Step: 4500, Loss: 2.356776\n",
      "Step: 4600, Loss: 2.291292\n",
      "Step: 4700, Loss: 2.313397\n",
      "Step: 4800, Loss: 2.327075\n",
      "Step: 4900, Loss: 2.311267\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.116\n"
     ]
    }
   ],
   "source": [
    "# Run 5\n",
    "\n",
    "sess_5 = tf.Session()\n",
    "\n",
    "weight_dim_5 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w51'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w52'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w53'),\n",
    "    'w4': def_weight([1024, num_classes], 'w54')\n",
    "}\n",
    "\n",
    "bias_dim_5= {\n",
    "    'b1': def_bias([32], 'b51'),\n",
    "    'b2': def_bias([64], 'b52'),\n",
    "    'b3': def_bias([1024], 'b53'),\n",
    "    'b4': def_bias([num_classes], 'b54')\n",
    "}\n",
    "\n",
    "runs_5 = 5000\n",
    "\n",
    "minibatch_5 = 16\n",
    "learnRate_5 = 1e-2\n",
    "gamma_5 = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "model_5 = ConvANN(sess_5, weight_dim_5, bias_dim_5, runs_5, minibatch_5, learnRate_5, gamma_5)\n",
    "\n",
    "model_5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 2.178702\n",
      "Step: 1000, Loss: 0.067084\n",
      "Step: 2000, Loss: 0.026480\n",
      "Step: 3000, Loss: 0.021406\n",
      "Step: 4000, Loss: 0.030152\n",
      "Step: 5000, Loss: 0.026771\n",
      "Step: 6000, Loss: 0.050622\n",
      "Step: 7000, Loss: 0.027832\n",
      "Step: 8000, Loss: 0.032912\n",
      "Step: 9000, Loss: 0.034477\n",
      "Step: 10000, Loss: 0.022863\n",
      "Step: 11000, Loss: 0.112639\n",
      "Step: 12000, Loss: 0.023649\n",
      "Step: 13000, Loss: 0.022876\n",
      "Step: 14000, Loss: 0.023728\n",
      "Step: 15000, Loss: 0.023915\n",
      "Step: 16000, Loss: 0.022817\n",
      "Step: 17000, Loss: 0.021602\n",
      "Step: 18000, Loss: 0.020760\n",
      "Step: 19000, Loss: 0.021752\n",
      "Optimization complete!\n",
      "Validation Accuracy:  0.995\n"
     ]
    }
   ],
   "source": [
    "# Run 6\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_6 = tf.Session()\n",
    "\n",
    "weight_dim_6 = {\n",
    "    'w1': def_weight([5, 5, 1, 32], 'w_61'),\n",
    "    'w2': def_weight([5, 5, 32, 64], 'w_62'),\n",
    "    'w3': def_weight([7 * 7 * 64, 1024], 'w_63'),\n",
    "    'w4': def_weight([1024, num_classes], 'w_64')\n",
    "}\n",
    "\n",
    "bias_dim_6 = {\n",
    "    'b1': def_bias([32], 'b_61'),\n",
    "    'b2': def_bias([64], 'b_62'),\n",
    "    'b3': def_bias([1024], 'b_63'),\n",
    "    'b4': def_bias([num_classes], 'b_64')\n",
    "}\n",
    "\n",
    "runs_6 = 20000\n",
    "\n",
    "minibatch_6 = 50\n",
    "learnRate_6 = 1e-3\n",
    "gamma_6 = 1e-5\n",
    "\n",
    "\n",
    "\n",
    "model_6 = ConvANN(sess_6, weight_dim_6, bias_dim_6, runs_6, minibatch_6, learnRate_6, gamma_6, display_steps=1000)\n",
    "\n",
    "model_6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.988\n"
     ]
    }
   ],
   "source": [
    "model_6.test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
